{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 384\n",
    "MODEL_ID = \"llava-hf/llava-1.5-7b-hf\"\n",
    "REPO_ID = \"thashiguchi/llava-finetuning-demo\"\n",
    "WANDB_PROJECT = \"LLaVa\"\n",
    "WANDB_NAME = \"llava-demo-cord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"naver-clova-ix/cord-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c51c5270b13460abf38711407af88e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "model = LlavaForConditionalGeneration.from_pretrained(MODEL_ID, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    images = [example[\"image\"] for example in examples[\"image\"]]\n",
    "    texts = [f\"USER: <image>\\nExtract JSON.\\nASSISTANT: {json.loads(gt)['gt_parse']}\" for gt in examples[\"ground_truth\"]]\n",
    "\n",
    "    inputs = processor(text=texts, images=images, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォーマット関数を追加\n",
    "def formatting_func(example):\n",
    "    try:\n",
    "        ground_truth = example['ground_truth']\n",
    "        if isinstance(ground_truth, list):\n",
    "            ground_truth = ground_truth[0]  # リストの場合、最初の要素を使用\n",
    "        parsed_gt = json.loads(ground_truth)\n",
    "        formatted_text = f\"USER: <image>\\nExtract JSON.\\nASSISTANT: {parsed_gt['gt_parse']}\"\n",
    "        return [formatted_text]  # リストとして返す\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example: {e}\")\n",
    "        print(f\"Example structure: {example}\")\n",
    "        raise  # エラーを再度発生させて処理を停止\n",
    "\n",
    "def custom_data_collator(data):\n",
    "    input_ids = [f[\"input_ids\"] for f in data if \"input_ids\" in f]\n",
    "    attention_mask = [f[\"attention_mask\"] for f in data if \"attention_mask\" in f]\n",
    "    pixel_values = [f[\"pixel_values\"] for f in data if \"pixel_values\" in f]\n",
    "\n",
    "    if input_ids:\n",
    "        input_ids = torch.stack(input_ids)\n",
    "    if attention_mask:\n",
    "        attention_mask = torch.stack(attention_mask)\n",
    "    if pixel_values:\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids if input_ids else None,\n",
    "        \"attention_mask\": attention_mask if attention_mask else None,\n",
    "        \"pixel_values\": pixel_values if pixel_values else None\n",
    "    }\n",
    "\n",
    "def custom_data_collator(data):\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    pixel_values = []\n",
    "\n",
    "    for f in data:\n",
    "        if \"input_ids\" in f:\n",
    "            input_ids.append(f[\"input_ids\"])\n",
    "        if \"attention_mask\" in f:\n",
    "            attention_mask.append(f[\"attention_mask\"])\n",
    "        if \"pixel_values\" in f:\n",
    "            pixel_values.append(f[\"pixel_values\"])\n",
    "\n",
    "    batch = {}\n",
    "\n",
    "    if input_ids:\n",
    "        batch[\"input_ids\"] = torch.stack(input_ids)\n",
    "    if attention_mask:\n",
    "        batch[\"attention_mask\"] = torch.stack(attention_mask)\n",
    "    if pixel_values:\n",
    "        batch[\"pixel_values\"] = torch.stack(pixel_values)\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
