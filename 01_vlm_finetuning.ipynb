{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wandb.ai/byyoung3/mlnews3/reports/How-to-fine-tune-Phi-3-Vision-on-a-custom-dataset--Vmlldzo4MTEzMTg3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1f8ba07cb64ef39ea9b35470e12124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb031fa881da48ad8e176acecd2b79cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/287k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559a06364ddf4d8db9194592123d83c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3038 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download N.A.: Invalid URL 'N.A.': No scheme supplied. Perhaps you meant https://N.A.?\n",
      "Failed to download N.A.: Invalid URL 'N.A.': No scheme supplied. Perhaps you meant https://N.A.?\n",
      "Dataset and images saved to ./data/burberry_dataset\n"
     ]
    }
   ],
   "source": [
    "# Function to download an image from a URL and save it locally\n",
    "def download_image(image_url, save_path):\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        image.save(save_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {image_url}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Download the dataset from Hugging Face\n",
    "dataset = load_dataset('DBQ/Burberry.Product.prices.United.States')\n",
    "\n",
    "\n",
    "# Convert the Hugging Face dataset to a Pandas DataFrame\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "\n",
    "# Create directories to save the dataset and images\n",
    "dataset_dir = './data/burberry_dataset'\n",
    "images_dir = os.path.join(dataset_dir, 'images')\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Filter out rows where image download fails\n",
    "filtered_rows = []\n",
    "for idx, row in df.iterrows():\n",
    "    image_url = row['imageurl']\n",
    "    image_name = f\"{row['product_code']}.jpg\"\n",
    "    image_path = os.path.join(images_dir, image_name)\n",
    "    if download_image(image_url, image_path):\n",
    "        row['local_image_path'] = image_path\n",
    "        filtered_rows.append(row)\n",
    "\n",
    "\n",
    "# Create a new DataFrame with the filtered rows\n",
    "filtered_df = pd.DataFrame(filtered_rows)\n",
    "\n",
    "\n",
    "# Save the updated dataset to disk\n",
    "dataset_path = os.path.join(dataset_dir, 'burberry_dataset.csv')\n",
    "filtered_df.to_csv(dataset_path, index=False)\n",
    "\n",
    "\n",
    "print(f\"Dataset and images saved to {dataset_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtomoya1995\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97039842286c40a7b9cc77bdc16f2c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112402400006735, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/workspace/thashiguchi/wandb_test/wandb/run-20241011_052610-bdkptne0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tomoya1995/vlm_finetuning/runs/bdkptne0' target=\"_blank\">burberry_product_phi3</a></strong> to <a href='https://wandb.ai/tomoya1995/vlm_finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tomoya1995/vlm_finetuning' target=\"_blank\">https://wandb.ai/tomoya1995/vlm_finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tomoya1995/vlm_finetuning/runs/bdkptne0' target=\"_blank\">https://wandb.ai/tomoya1995/vlm_finetuning/runs/bdkptne0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.11.7/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 102\u001b[0m\n\u001b[1;32m     96\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[1;32m    105\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/transformers/modeling_utils.py:3826\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3823\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   3825\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[0;32m-> 3826\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3827\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3828\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3831\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[1;32m   3832\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(config, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/transformers/modeling_utils.py:1556\u001b[0m, in \u001b[0;36mPreTrainedModel._autoset_attn_implementation\u001b[0;34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[0m\n\u001b[1;32m   1553\u001b[0m     config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attention_2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1556\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   1564\u001b[0m     \u001b[38;5;66;03m# use_flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.\u001b[39;00m\n\u001b[1;32m   1565\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_check_and_enable_sdpa(\n\u001b[1;32m   1566\u001b[0m         config,\n\u001b[1;32m   1567\u001b[0m         hard_check_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m requested_attn_implementation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1568\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.7/lib/python3.11/site-packages/transformers/modeling_utils.py:1658\u001b[0m, in \u001b[0;36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[0;34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[0m\n\u001b[1;32m   1655\u001b[0m install_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1660\u001b[0m flash_attention_version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda:\n",
      "\u001b[0;31mImportError\u001b[0m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import resize, to_pil_image\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/workspace/thashiguchi/api\")\n",
    "sys.path.append(\"/home/ubuntu/workspace/thashiguchi/configs\")\n",
    "import config\n",
    "wandb.login(key=config.WANDB_API_KEY) # Pass your W&B API key here\n",
    "run = wandb.init(project=\"vlm_finetuning\", name=\"burberry_product_phi3\") # プロジェクト名とnameを指定\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(3)\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset for Burberry Product Prices and Images\n",
    "class BurberryProductDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length, image_size):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.padding_side = 'left'\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = f\"<|user|>\\n<|image_1|>What is shown in this image?<|end|><|assistant|>\\nProduct: {row['title']}, Category: {row['category3_code']}, Full Price: {row['full_price']}<|end|>\"\n",
    "        image_path = row['local_image_path']\n",
    "        \n",
    "        # Tokenize text\n",
    "        encodings = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        \n",
    "        try:\n",
    "            # Load and transform image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = self.image_transform_function(image)\n",
    "        except (FileNotFoundError, IOError):\n",
    "            # Skip the sample if the image is not found\n",
    "            return None\n",
    "        \n",
    "        encodings['pixel_values'] = image\n",
    "        encodings['price'] = row['full_price']\n",
    "        \n",
    "        return {key: torch.tensor(val) for key, val in encodings.items()}\n",
    "\n",
    "\n",
    "    def image_transform_function(self, image):\n",
    "        image = np.array(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset from disk\n",
    "dataset_path = './data/burberry_dataset/burberry_dataset.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "\n",
    "# Initialize processor and tokenizer\n",
    "model_id = \"microsoft/Phi-3-vision-128k-instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.9 * len(df))\n",
    "val_size = len(df) - train_size\n",
    "train_indices, val_indices = random_split(range(len(df)), [train_size, val_size])\n",
    "train_indices = train_indices.indices\n",
    "val_indices = val_indices.indices\n",
    "train_df = df.iloc[train_indices]\n",
    "val_df = df.iloc[val_indices]\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = BurberryProductDataset(train_df, tokenizer, max_length=512, image_size=128)\n",
    "val_dataset = BurberryProductDataset(val_df, tokenizer, max_length=512, image_size=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True, torch_dtype=\"auto\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "eval_interval = 150  # Evaluate every 'eval_interval' steps\n",
    "loss_scaling_factor = 1000.0  # Variable to scale the loss by a certain amount\n",
    "save_dir = './saved_models'\n",
    "step = 0\n",
    "accumulation_steps = 64  # Accumulate gradients over this many steps\n",
    "\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = None\n",
    "\n",
    "\n",
    "# Select 10 images from the validation set for logging\n",
    "num_log_samples = 10\n",
    "log_indices = random.sample(range(len(val_dataset)), num_log_samples)\n",
    "\n",
    "\n",
    "def extract_price_from_predictions(predictions, tokenizer):\n",
    "    # Assuming the price is at the end of the text and separated by a space\n",
    "    predicted_text = tokenizer.decode(predictions[0], skip_special_tokens=True)\n",
    "    try:\n",
    "        predicted_price = float(predicted_text.split()[-1].replace(',', ''))\n",
    "    except ValueError:\n",
    "        predicted_price = 0.0\n",
    "    return predicted_price\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, device, tokenizer, step, log_indices, max_samples=None, ):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_price_error = 0\n",
    "    log_images = []\n",
    "    log_gt_texts = []\n",
    "    log_pred_texts = []\n",
    "    table = wandb.Table(columns=[\"Image\", \"Ground Truth Text\", \"Predicted Text\"])\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            if max_samples and i >= max_samples:\n",
    "                break\n",
    "\n",
    "\n",
    "            if batch is None:  # Skip if the batch is None\n",
    "                continue\n",
    "\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = input_ids.clone().detach()\n",
    "            actual_price = batch['price'].item()\n",
    "\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                pixel_values=pixel_values, \n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "            # Calculate price error\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            predicted_price = extract_price_from_predictions(predictions, tokenizer)\n",
    "            price_error = abs(predicted_price - actual_price)\n",
    "            total_price_error += price_error\n",
    "\n",
    "\n",
    "            # Log images, ground truth texts, and predicted texts\n",
    "            if i in log_indices:\n",
    "                log_images.append(pixel_values.cpu().squeeze().numpy())\n",
    "                log_gt_texts.append(tokenizer.decode(labels[0], skip_special_tokens=True))\n",
    "                log_pred_texts.append(tokenizer.decode(predictions[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "                # Convert image to PIL format\n",
    "                pil_img = to_pil_image(resize(torch.from_numpy(log_images[-1]).permute(2, 0, 1), (336, 336))).convert(\"RGB\")\n",
    "                \n",
    "                # Add data to the table\n",
    "                table.add_data(wandb.Image(pil_img), log_gt_texts[-1], log_pred_texts[-1])\n",
    "\n",
    "\n",
    "                # Log the table incrementally\n",
    "    \n",
    "    wandb.log({\"Evaluation Results step {}\".format(step): table, \"Step\": step})\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / (i + 1)  # i+1 to account for the loop index\n",
    "    avg_price_error = total_price_error / (i + 1)\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    return avg_loss, avg_price_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):  # Number of epochs\n",
    "    total_train_loss = 0\n",
    "    total_train_price_error = 0\n",
    "    batch_count = 0\n",
    "\n",
    "\n",
    "    for batch in train_loader:\n",
    "        step += 1\n",
    "\n",
    "\n",
    "        if batch is None:  # Skip if the batch is None\n",
    "            continue\n",
    "\n",
    "\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        pixel_values = batch['pixel_values'].to(model.device)\n",
    "        labels = input_ids.clone().detach()\n",
    "        actual_price = batch['price'].float().to(model.device)\n",
    "\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            pixel_values=pixel_values, \n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        total_loss = loss\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)            \n",
    "        predicted_price = extract_price_from_predictions(predictions, tokenizer)\n",
    "\n",
    "\n",
    "        \n",
    "        total_loss.backward()\n",
    "\n",
    "\n",
    "        if (step % accumulation_steps) == 0:\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad /= accumulation_steps\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        total_train_loss += total_loss.item()\n",
    "        total_train_price_error += abs(predicted_price - actual_price.item())\n",
    "        batch_count += 1\n",
    "\n",
    "\n",
    "        # Log batch loss to wandb\n",
    "        wandb.log({\"Batch Loss\": total_loss.item(), \"Step\": step})\n",
    "\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Step: {step}, Batch Loss: {total_loss.item()}\")\n",
    "\n",
    "\n",
    "        if step % eval_interval == 0:\n",
    "            val_loss, val_price_error = evaluate(model, val_loader, model.device, tokenizer=tokenizer, log_indices=log_indices, step=step )\n",
    "            wandb.log({\n",
    "                \"Validation Loss\": val_loss,\n",
    "                \"Validation Price Error (Average)\": val_price_error,\n",
    "                \"Step\": step\n",
    "            })\n",
    "            print(f\"Step: {step}, Validation Loss: {val_loss}, Validation Price Error (Normalized): {val_price_error}\")\n",
    "\n",
    "\n",
    "            # Save the best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_path = os.path.join(save_dir, f\"best_model\")\n",
    "                model.save_pretrained(best_model_path, safe_serialization=False)\n",
    "                tokenizer.save_pretrained(best_model_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            avg_train_loss = total_train_loss / batch_count\n",
    "            avg_train_price_error = total_train_price_error / batch_count\n",
    "            wandb.log({\n",
    "                \"Epoch\": epoch,\n",
    "                \"Average Training Loss\": avg_train_loss,\n",
    "                \"Average Training Price Error\": avg_train_price_error\n",
    "            })\n",
    "            \n",
    "    print(f\"Epoch: {epoch}, Average Training Loss: {avg_train_loss}, Average Training Price Error: {avg_train_price_error}\")\n",
    "\n",
    "\n",
    "    if best_model_path:\n",
    "        run.log_model(\n",
    "            path=best_model_path,\n",
    "            name=\"phi3-v-burberry\",\n",
    "            aliases=[\"best\"],\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
